{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification de la paraphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problématique \n",
    "Il s'agit de dire si deux phrases partagent le même sens, ou autrement dit si elles sont paraphrases l'une de l'autre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/luciegabagnou/Documents/MOSEF/ML_AVANCE/PROJET/paraphrase-identification/notebooks\n",
      "Requirement already satisfied: datasets in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.10.1)\n",
      "Requirement already satisfied: pandas in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: numpy in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.24.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: nltk in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: matplotlib in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (3.7.1)\n",
      "Requirement already satisfied: gensim in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.3.1)\n",
      "Requirement already satisfied: seaborn in /Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.12.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation de l'env\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "# Installation des packages nécessaires à ce projet\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciegabagnou/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/luciegabagnou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/luciegabagnou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from scripts.data_utils import HuggingFaceExtracting,ETLPipeline,WordEmbedding\n",
    "from scripts.model import SiameseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importation des données\n",
    "\n",
    "Ici, pour collecter les données des données nous avons créée une classe **HuggingFaceExtracting** : une classe pour extraire des données à partir de datasets Hugging Face. Elle comprend des méthodes pour extraire les features et les labels, charger des données de datasets Hugging Face, et exporter des données dans un fichier CSV (ou laisser en DataFrame pandas).\n",
    "Pour rendre uniforme les inputs dans la pipeline de preprocessing, nous avons crée une colonne \"inputs\" composée des paires de phrases sous forme de liste, et une \"labels\" ayant les labels bien encodés.\n",
    "\n",
    "\n",
    "A l'origine:\n",
    "On a effectué une analyse sur l'ensemble du jeu de données visible sur [HuggingFace](https://huggingface.co/datasets/bigscience/P3/viewer/glue_qqp_same_thing/). Les colonnes qui vous nous intéressez sont \"inputs_pretokenized\" et \"targets_pretokenized\":\n",
    "- \"inputs_pretokenized\":'Are the questions \"How is the life of a math student? Could you describe your own experiences?\" and \"Which level of prepration is enough for the exam jlpt5?\" asking the same thing? '\n",
    "=> Il s'agit pour chaque observation d'une paire de phrases jointes par \"and\" \n",
    "=> Remarque: il s'agit de questions pour l'ensemble des observations\n",
    "- \"targets_pretokenized\": est-ce que la paire de phrases précédente est une paraphrase ou non? \"yes\" => Il s'agit bien d'une paraphrase et \"no\" les phrases ne sont pas des paraphrases\n",
    "\n",
    "\n",
    "A la fin, on a:\n",
    "\n",
    "- \"inputs\": [\"How is the life of a math student? Could you describe your own experiences?\",\"Which level of prepration is enough for the exam jlpt5?\"] obtenu en utilisant une regex adapté au contexte\n",
    "- \"labels\": On a encodé les labels en {0,1}: 0 => pas de paraphrase , 1 => paraphrase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset p3 (/Users/luciegabagnou/.cache/huggingface/datasets/bigscience___p3/glue_qqp_same_thing/0.1.0/204f22caf7f0cbaf01a8631ec396c1cab69f8d71f276fb8619fae696536874ab)\n",
      "100%|██████████| 3/3 [00:00<00:00, 203.58it/s]\n"
     ]
    }
   ],
   "source": [
    "huggingface_extractor = HuggingFaceExtracting(\"bigscience/P3\",\"glue_qqp_same_thing\")\n",
    "huggingface_extractor.load_huggingface_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = huggingface_extractor.extract(\"train\")\n",
    "test = huggingface_extractor.extract(\"test\")\n",
    "validation = huggingface_extractor.extract(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[How is the life of a math student? Could you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[How do I control my horny emotions?, How do y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[What causes stool color to change to yellow?,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[What can one do after MBBS?, What do i do aft...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Where can I find a power outlet for my laptop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  labels\n",
       "0  [How is the life of a math student? Could you ...       0\n",
       "1  [How do I control my horny emotions?, How do y...       1\n",
       "2  [What causes stool color to change to yellow?,...       0\n",
       "3  [What can one do after MBBS?, What do i do aft...       1\n",
       "4  [Where can I find a power outlet for my laptop...       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA \n",
    "Il y a 363846 observations (paires de phrases + targets) dans l'échantillon de train, 390965 dans l'échantillon de test, et 40430 dans l'échantillon de validation. Au total, on compte donc 795241 observations.\n",
    "\n",
    "Dans un premier temps, on a réalisé une analyse exploratoire du jeu de données (dans sa globalité) pour comprendre quel était le type de données auquel nous étions confronté. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train,test,validation],axis=0).reset_index()\n",
    "sentences = data[\"inputs\"].values.tolist()\n",
    "sentences=flatten(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots median dans la phrase 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lenght=pd.Series(sentences).apply(lambda x: len(x.split()))\n",
    "sns.histplot(lenght)\n",
    "print(\"Nombre de mots median dans la phrase\",round(lenght.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.590482e+06\n",
       "mean     1.112469e+01\n",
       "std      5.822297e+00\n",
       "min      0.000000e+00\n",
       "25%      7.000000e+00\n",
       "50%      1.000000e+01\n",
       "75%      1.300000e+01\n",
       "max      2.370000e+02\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenght.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthèse de l'analyse exploratoire\n",
    "- Syntaxe : Puisque toutes les observations sont des questions, leur syntaxe devrait être plutôt similaire. Ainsi, la syntaxe n'est pas un facteur qu'il faut inclure dans notre analyse en tant que caractéristique. Cependant, si nous observions une variété de structures, cela pourrait créer de l'hétérogénéité dans les observations. Dans ce cas, nous devrions réaligner toutes les phrases pour qu'elles aient une structure similaire afin de les comparer. Par conséquent, nous pouvons simplifier notre analyse en nous concentrant sur le contenu sémantique des questions plutôt que sur leur structure syntaxique, qui est relativement simple dans ce cas. \n",
    "- Les mots: La longueur des phrases est plutôt homogène et est concentré autour de 10 mots et 75% des phrases ont une longueur inférieure à 13 mots. En ce qui concerne le lexique, nous ne devrons pas nous focaliser dessus pour discriminer les observations les unes des autres (contrairement à la classification de documents qui se concentre sur le vocabulaire). \n",
    "- Langue: Le modèle sera propre à l'anglais vu qu'il s'agit de l'unique langue employé dans les phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Pour cette partie, nous avons créé une classe **ETLPipeline** : une classe représentant un pipeline ETL pour les datasets de texte. Elle comprend des méthodes pour normaliser les entrées, transformer les paires de phrases et les étiquettes en entrées appropriées pour un modèle de classification de texte, et exporter des données.\n",
    "La fonction principale qui traite le processing du texte est:\n",
    "\n",
    "\n",
    "```python\n",
    "def standardize_text_inputs(self, input_data):\n",
    "    self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def process_sentence(sentence):\n",
    "        sentence = contractions.fix(sentence)\n",
    "        sentence = sentence.replace('\"', \"\").replace('?', \"\")\n",
    "        tokens = nltk.word_tokenize(sentence.lower())\n",
    "        if self.lemmatize_option is True:\n",
    "            return [self.lemmatizer.lemmatize(word) for word in tokens if word.isalpha()]\n",
    "        else:\n",
    "            return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    all_tokens = [process_sentence(sentence) for sentence in input_data]\n",
    "\n",
    "    if self.join_option:\n",
    "        all_tokens = [\" \".join(tokens) for tokens in all_tokens]\n",
    "        \n",
    "    return all_tokens\n",
    "        \n",
    "```\n",
    "Comme on peut le voir, elle effectue différents étapes: \n",
    "- enlève les contractions du type \"I'm\" et les convertit en \"I am\"\n",
    "- remplace les signes de ponctuations parasites tel que \"\"\"\" et \"?\"\n",
    "- met en minuscules les caractères et découpe en tokens les mots (selon nltk)\n",
    "- si souhaité, la fonction lemmatise ce qui peut permettre d'uniformiser les mots en cas de pluriel, conjugaison..\n",
    "- enfin on garde uniquement les élements sont des lettres\n",
    "- selon le word embedding utilisé, on laisse le choix à l'utilisateur de s'il souhaite récupérer la phrase préprocessé sous forme de liste de tokens ou bien d'une phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ETLPipeline()\n",
    "#La \"méthode\" exécute l'ensemble de la pipeline\n",
    "train_data = preprocessor.transform(train)\n",
    "test_data = preprocessor.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "Dans cette partie, nous avons utilisé la classe **WordEmbedding**: elle est utilisée pour créer une représentation vectorielle pour les mots en utilisant un modèle d'embedding de mots pré-entraîné. La classe possède des méthodes:\n",
    "-  pour fit un tokenizer sur les données d'entraînement (ensemble des phrases)\n",
    "- pour tokenizer et padder (vectoriser et uniformiser la longueur) des séquences d'entrée (sur les phrases 1 puis 2 respectivement)\n",
    "- pour charger le modèle pré-entraîné (qui devront être accessibles dans models)\n",
    "- pour construire une matrice de poids qui sera utilisée pour initialiser les poids de la couche d'embedding dans un modèle de réseau de neurones: il s'agit d'une couche partagée entre les deux inputs (les deux phrases d'entrée) et les mêmes poids sont utilisés pour vectoriser toutes les phrases (même si c'est sur 2 branches différentes).\n",
    "\n",
    "Ici, nous faisons un zoom sur la construction de la matrice des poids:\n",
    "\n",
    "```` python\n",
    "def build_weight_matrix(self):\n",
    "    vocab_size = len(self.tokenizer.word_index) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, self.embedding_size))\n",
    "    for word, i in self.tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = self.pretrained_model[word]\n",
    "            weight_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            weight_matrix[i] = np.random.uniform(-5, 5, self.embedding_size)\n",
    "    return weight_matrix\n",
    "````\n",
    "Comme on peut le voir, la fonction construit une matrice de poids à partir d'un modèle d'embedding de mots pré-entraîné et d'un tokenizer. La matrice de poids est utilisée pour initialiser les poids de la couche d'embedding dans un modèle de réseau de neurones.\n",
    "Plus précisément, pour chaque mot dans le vocabulaire, la méthode tente de récupérer son vecteur d'embedding à partir du modèle d'embedding pré-entraîné. Si le mot est présent dans le modèle, la méthode utilise son vecteur d'embedding comme poids pour la ligne correspondante dans la matrice de poids. Si le mot n'est pas présent dans le modèle, la méthode initialise une ligne aléatoire de la matrice de poids avec des valeurs uniformément distribuées entre -5 et 5.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on a utilisé un modèle FastText (explication dans le rapport) car il semblait le plus adapté pour traiter les mots \"hors vocabulaire\" c'est-à-dire les mots non connus lors de la prédiction (si typiquement un mot d'une prédiction n'était pas présent dans l'échantillon d'apprentissage). \n",
    "Nous tenterons plus tard de changer de modèle en utilisant Word2Vec pour voir si cela peut améliorer les performances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on effectue différentes étapes:\n",
    "- On instancie l'objet qui fait le word-embedding en indiquant le type de modèle pré-entraîné et le chemin d'accès du modèle utilisé\n",
    "- On ajuste le tokenizer propre au modèle à partir du corpus de texte d'entraînement (il s'agit d'une vectorisation simple)\n",
    "- On charge le modèle pré-entrainé \n",
    "- On récupère les inputs tokenizés et uniformisés en longueur\n",
    "- On créée une matrice de poids basé sur la fonction vu précédemment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_embedder = WordEmbedding(15,\"FastText\",\"cc.en.100.bin\")\n",
    "word_embedder.tokenizer(train_data[\"sentences_1\"]+ train_data[\"sentences_2\"])\n",
    "word_embedder.load_pretrained_model()\n",
    "train_sentences_1 = word_embedder.tokenize_and_pad(train_data[\"sentences_1\"])\n",
    "train_sentences_2 = word_embedder.tokenize_and_pad(train_data[\"sentences_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix = word_embedder.build_weight_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fait la même chose pour l'échantillon de test\n",
    "test_data = preprocessor.transform(test)\n",
    "test_sentences_1 = word_embedder.tokenize_and_pad(test_data[\"sentences_1\"])\n",
    "test_sentences_2 = word_embedder.tokenize_and_pad(test_data[\"sentences_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project my_dir/my_project/oracle.json\n",
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 07:43:24.571300: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-27 07:43:24.571401: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from my_dir/my_project/tuner0.json\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "96                |256               |lstm_layers\n",
      "rmsprop           |rmsprop           |optimizer\n",
      "1e-05             |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 07:43:25.757600: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-27 07:43:26.948914: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:43:27.262295: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:43:27.262769: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:43:27.692539: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:43:27.713331: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19/11371 [..............................] - ETA: 9:02 - loss: 0.3013 - accuracy: 0.5641"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m stop_early \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[39m# Search for the best hyperparameters\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(\n\u001b[1;32m     64\u001b[0m     [train_sentences_1,train_sentences_2],\n\u001b[1;32m     65\u001b[0m     np\u001b[39m.\u001b[39;49marray(train_data[\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[1;32m     66\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     67\u001b[0m     validation_data\u001b[39m=\u001b[39;49m([test_sentences_1,test_sentences_2], np\u001b[39m.\u001b[39;49marray(test_data[\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m])),\n\u001b[1;32m     68\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[stop_early]\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[39m# Get the best hyperparameters\u001b[39;00m\n\u001b[1;32m     72\u001b[0m best_hps \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_hyperparameters(num_trials\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py:183\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 183\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_trial(trial, \u001b[39m*\u001b[39;49mfit_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m    184\u001b[0m \u001b[39m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/keras_tuner/engine/tuner.py:295\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    294\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[0;32m--> 295\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_and_fit_model(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcopied_kwargs)\n\u001b[1;32m    297\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/keras_tuner/engine/tuner.py:222\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[1;32m    221\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 222\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mfit(hp, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    223\u001b[0m tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[1;32m    224\u001b[0m     results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mHyperModel.fit()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/keras_tuner/engine/hypermodel.py:140\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda/envs/dl_gpu/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trial 5 Complete [01h 07m 47s]\n",
    "val_accuracy: 0.9359405636787415\n",
    "Pour LSTM simple\n",
    "Best val_accuracy So Far: 0.9359405636787415\n",
    "Total elapsed time: 05h 43m 29s\n",
    "INFO:tensorflow:Oracle triggered exit\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "import keras.backend as K\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    y_true = tf.cast(y_true, tf.float32) # convertir y_true en float32\n",
    "    return tf.keras.backend.mean(y_true * tf.keras.backend.square(y_pred) + (1 - y_true) * tf.keras.backend.square(tf.keras.backend.maximum(margin - y_pred, 0)))\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "def manhattan_distance(A, B):\n",
    "    return tf.keras.backend.sum(tf.keras.backend.abs(A - B), axis=1, keepdims=True)\n",
    "\n",
    "def build_model(hp):\n",
    "    input_dim = 15\n",
    "    input_dim_embedding = weight_matrix.shape[0]\n",
    "    output_dim_embedding = weight_matrix.shape[1]\n",
    "    distance_layer_function = manhattan_distance\n",
    "\n",
    "    input1 = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    input2 = tf.keras.layers.Input(shape=(input_dim,))\n",
    "\n",
    "    shared_embedding = tf.keras.layers.Embedding(input_dim=input_dim_embedding, output_dim=output_dim_embedding, weights=[weight_matrix])\n",
    "    shared_lstm = tf.keras.layers.LSTM(hp.Int('lstm_layers', min_value=32, max_value=256, step=32))\n",
    "    encoded1 = shared_lstm(shared_embedding(input1))\n",
    "    encoded1 = tf.keras.layers.BatchNormalization()(encoded1)\n",
    "    encoded2 = shared_lstm(shared_embedding(input2))\n",
    "    encoded2 = tf.keras.layers.BatchNormalization()(encoded2)\n",
    "    merged_vector = tf.keras.layers.Lambda(lambda x: distance_layer_function(x[0], x[1]))([encoded1, encoded2])\n",
    "    output = tf.keras.layers.BatchNormalization()(merged_vector)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(output)\n",
    "    siamese_net = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5]))\n",
    "    siamese_net.compile(loss=contrastive_loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "# Split data into training and validation sets\n",
    "\n",
    "# Create a tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='my_dir',\n",
    "    project_name='my_project')\n",
    "# Define a callback to stop training if validation accuracy stops improving\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(\n",
    "    [train_sentences_1,train_sentences_2],\n",
    "    np.array(train_data[\"label\"]),\n",
    "    epochs=5,\n",
    "    validation_data=([test_sentences_1,test_sentences_2], np.array(test_data[\"label\"])),\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=10)[0]\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "import keras.backend as K\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    y_true = tf.cast(y_true, tf.float32) # convertir y_true en float32\n",
    "    return tf.keras.backend.mean(y_true * tf.keras.backend.square(y_pred) + (1 - y_true) * tf.keras.backend.square(tf.keras.backend.maximum(margin - y_pred, 0)))\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "def manhattan_distance(A, B):\n",
    "    return tf.keras.backend.sum(tf.keras.backend.abs(A - B), axis=1, keepdims=True)\n",
    "\n",
    "def build_model(hp):\n",
    "    input_dim = 15\n",
    "    input_dim_embedding = weight_matrix.shape[0]\n",
    "    output_dim_embedding = weight_matrix.shape[1]\n",
    "    distance_layer_function = manhattan_distance\n",
    "\n",
    "    input1 = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    input2 = tf.keras.layers.Input(shape=(input_dim,))\n",
    "\n",
    "    shared_embedding = tf.keras.layers.Embedding(input_dim=input_dim_embedding, output_dim=output_dim_embedding, weights=[weight_matrix])\n",
    "    shared_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp.Int('lstm_layers', min_value=32, max_value=256, step=32)))\n",
    "    encoded1 = shared_lstm(shared_embedding(input1))\n",
    "    encoded1 = tf.keras.layers.BatchNormalization()(encoded1)\n",
    "    encoded2 = shared_lstm(shared_embedding(input2))\n",
    "    encoded2 = tf.keras.layers.BatchNormalization()(encoded2)\n",
    "    merged_vector = tf.keras.layers.Lambda(lambda x: distance_layer_function(x[0], x[1]))([encoded1, encoded2])\n",
    "    output = tf.keras.layers.BatchNormalization()(merged_vector)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(output)\n",
    "    siamese_net = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "        # Choix de l'optimiseur et du learning rate\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    siamese_net.compile(loss=contrastive_loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "# Split data into training and validation sets\n",
    "\n",
    "# Create a tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=2,\n",
    "    directory='my_dir',\n",
    "    project_name='my_project')\n",
    "# Define a callback to stop training if validation accuracy stops improving\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(\n",
    "    [train_sentences_1,train_sentences_2],\n",
    "    np.array(train_data[\"label\"]),\n",
    "    epochs=5,\n",
    "    validation_data=([test_sentences_1,test_sentences_2], np.array(test_data[\"label\"])),\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=10)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = best_hps.get('learning_rate')\n",
    "best_hidden_layer_lstm = best_hps.get('lstm_layers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 07:43:35.140711: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:43:35.460401: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:43:35.461531: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/11371 [..............................] - ETA: 6:15:29 - loss: 0.2790 - binary_crossentropy: 0.7295 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 07:43:35.793450: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:43:35.812480: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11371/11371 [==============================] - ETA: 0s - loss: 0.2759 - binary_crossentropy: 0.7148 - accuracy: 0.4988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 07:50:32.253382: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:50:32.381120: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-27 07:50:32.381139: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11371/11371 [==============================] - 522s 46ms/step - loss: 0.2759 - binary_crossentropy: 0.7148 - accuracy: 0.4988 - val_loss: 0.3031 - val_binary_crossentropy: 0.6692 - val_accuracy: 0.7334\n",
      "Epoch 2/10\n",
      "11371/11371 [==============================] - 516s 45ms/step - loss: 0.2444 - binary_crossentropy: 0.7628 - accuracy: 0.3973 - val_loss: 0.3340 - val_binary_crossentropy: 0.6005 - val_accuracy: 0.8764\n",
      "Epoch 3/10\n",
      "11371/11371 [==============================] - 515s 45ms/step - loss: 0.2358 - binary_crossentropy: 0.7804 - accuracy: 0.3737 - val_loss: 0.3617 - val_binary_crossentropy: 0.5388 - val_accuracy: 0.9048\n",
      "Epoch 4/10\n",
      "11371/11371 [==============================] - 517s 45ms/step - loss: 0.2316 - binary_crossentropy: 0.7958 - accuracy: 0.3657 - val_loss: 0.3739 - val_binary_crossentropy: 0.5244 - val_accuracy: 0.8997\n",
      "Epoch 5/10\n",
      "11371/11371 [==============================] - 518s 46ms/step - loss: 0.2281 - binary_crossentropy: 0.8120 - accuracy: 0.3584 - val_loss: 0.3906 - val_binary_crossentropy: 0.4979 - val_accuracy: 0.9036\n",
      "Epoch 6/10\n",
      "11371/11371 [==============================] - 512s 45ms/step - loss: 0.2247 - binary_crossentropy: 0.8300 - accuracy: 0.3502 - val_loss: 0.4023 - val_binary_crossentropy: 0.4834 - val_accuracy: 0.9083\n",
      "Epoch 7/10\n",
      "11371/11371 [==============================] - 505s 44ms/step - loss: 0.2216 - binary_crossentropy: 0.8482 - accuracy: 0.3432 - val_loss: 0.4183 - val_binary_crossentropy: 0.4614 - val_accuracy: 0.9136\n",
      "Epoch 8/10\n",
      "11371/11371 [==============================] - 507s 45ms/step - loss: 0.2168 - binary_crossentropy: 0.8726 - accuracy: 0.3326 - val_loss: 0.4253 - val_binary_crossentropy: 0.4619 - val_accuracy: 0.9013\n",
      "Epoch 9/10\n",
      " 5481/11371 [=============>................] - ETA: 3:31 - loss: 0.2135 - binary_crossentropy: 0.8920 - accuracy: 0.3254"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "def manhattan_distance(A, B):\n",
    "    return tf.keras.backend.sum(tf.keras.backend.abs(A - B), axis=1, keepdims=True)\n",
    "siamese_lstm_model= SiameseNet(max_len=15,activation_function_output=\"sigmoid\",distance_layer_function=manhattan_distance,hidden_layer_architecture= LSTM,hidden_layer_neurons=128,weight_matrix=weight_matrix)\n",
    "siamese_lstm_model_history =  siamese_lstm_model.fit(train_sentences_1,train_sentences_2,np.array(train_data[\"label\"]),test_sentences_1,test_sentences_2,np.array(test_data[\"label\"]),cost_function=contrastive_loss,optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import keras.backend as K\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "\n",
    "def Manhattan_distance(A,B):\n",
    "   return K.sum( K.abs( A-B),axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "def improved_siamese_cnn():\n",
    "    input1 = tf.keras.layers.Input(shape=(16,))\n",
    "    input2 = tf.keras.layers.Input(shape=(16,))\n",
    "\n",
    "    shared_embedding = tf.keras.layers.Embedding(input_dim=66193, output_dim=100, weights=[weight_matrix])\n",
    "\n",
    "    shared_conv = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(128, kernel_size=3, padding='same', activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.GlobalMaxPooling1D()\n",
    "    ])\n",
    "\n",
    "    encoded1 = shared_conv(shared_embedding(input1))\n",
    "    encoded2 = shared_conv(shared_embedding(input2))\n",
    "\n",
    "    merged_vector = tf.keras.layers.Lambda(lambda x: Manhattan_distance(x[0], x[1]))([encoded1, encoded2])\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n",
    "    siamese_net = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "improved_siamese_cnn_model = improved_siamese_cnn()\n",
    "\n",
    "# Compile the model\n",
    "improved_siamese_cnn_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['binary_crossentropy', 'binary_accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mod\n",
    "                                                      \n",
    "                                                      \n",
    "                                                      e='min')\n",
    "\n",
    "# Train the model\n",
    "improved_siamese_cnn_model.fit(\n",
    "    [train_sentences_1_, train_sentences_2_], np.array(train_labels),\n",
    "    batch_size=32, epochs=10, validation_split=0.2, verbose=1,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    text=nltk.word_tokenize(text)\n",
    "    tags = [tag for _, tag in nltk.pos_tag(text)]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_tags_1_ = [pos_tagging(sentence) for sentence in train_sentences_1]\n",
    "train_pos_tags_2_ = [pos_tagging(sentence) for sentence in train_sentences_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_seq_len = 16\n",
    "\n",
    "pos_tokenizer = Tokenizer()\n",
    "pos_tokenizer.fit_on_texts(train_pos_tags_1_ + train_pos_tags_2_)\n",
    "\n",
    "\n",
    "train_pos_tags_1_ = pos_tokenizer.texts_to_sequences(train_pos_tags_1_)\n",
    "train_pos_tags_2_ = pos_tokenizer.texts_to_sequences(train_pos_tags_2_)\n",
    "\n",
    "\n",
    "train_pos_tags_1_ = pad_sequences(train_pos_tags_1_, maxlen=max_seq_len)\n",
    "train_pos_tags_2_ = pad_sequences(train_pos_tags_2_, maxlen=max_seq_len)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def improved_siamese_lstm_with_pos():\n",
    "    \n",
    "    def attention_layer(encoded1, encoded2):\n",
    "        attention = tf.keras.layers.Dot(axes=(2, 2))([encoded1, encoded2])\n",
    "        attention = tf.keras.layers.Activation('softmax')(attention)\n",
    "        context = tf.keras.layers.Dot(axes=(2, 1))([attention, encoded2])\n",
    "        return context\n",
    "\n",
    "    input1 = tf.keras.layers.Input(shape=(16,))\n",
    "    input2 = tf.keras.layers.Input(shape=(16,))\n",
    "    pos_input1 = tf.keras.layers.Input(shape=(16,))\n",
    "    pos_input2 = tf.keras.layers.Input(shape=(16,))\n",
    "\n",
    "    shared_embedding = tf.keras.layers.Embedding(input_dim=66193, output_dim=100, weights=[weight_matrix])\n",
    "    shared_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "    pos_embedding = tf.keras.layers.Embedding(input_dim=len(pos_tokenizer.word_index) + 1, output_dim=10)\n",
    "\n",
    "    encoded1 = shared_lstm(shared_embedding(input1))\n",
    "    encoded1 = tf.keras.layers.BatchNormalization()(encoded1)\n",
    "    encoded2 = shared_lstm(shared_embedding(input2))\n",
    "    encoded2 = tf.keras.layers.BatchNormalization()(encoded2)\n",
    "\n",
    "    pos_encoded1 = pos_embedding(pos_input1)\n",
    "    pos_encoded2 = pos_embedding(pos_input2)\n",
    "\n",
    "    context1 = attention_layer(encoded1, encoded2)\n",
    "    context2 = attention_layer(encoded2, encoded1)\n",
    "\n",
    "    merged = tf.keras.layers.concatenate([encoded1, context1, pos_encoded1, encoded2, context2, pos_encoded2], axis=-1)\n",
    "    merged = tf.keras.layers.GlobalMaxPooling1D()(merged)\n",
    "    merged = tf.keras.layers.Dropout(0.5)(merged)\n",
    "    merged = tf.keras.layers.BatchNormalization()(merged)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    siamese_net = tf.keras.models.Model(inputs=[input1, input2, pos_input1, pos_input2], outputs=output)\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "improved_siamese_lstm_model_with_pos = improved_siamese_lstm_with_pos()\n",
    "\n",
    "# Compile the model\n",
    "improved_siamese_lstm_model_with_pos.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['binary_crossentropy', 'binary_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "\n",
    "improved_siamese_lstm_model_with_pos.fit(\n",
    "    [train_sentences_1_, train_sentences_2_,train_pos_tags_1_,train_pos_tags_2_], np.array(train_labels),\n",
    "    batch_size=32, epochs=10, validation_split=0.2, verbose=1,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('dl_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58139a97a23f8524c35064e91912c687e768b6a827197604e5a6b0cb6960b4eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
